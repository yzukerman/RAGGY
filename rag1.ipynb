{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbca798-b96f-4c7d-b526-93083389ee28",
   "metadata": {},
   "source": [
    "# RAG Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcf70d-9a01-46c6-8312-dfa02814c50a",
   "metadata": {},
   "source": [
    "To start, let's get GPT and its tokenizer set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffb40a1-6734-4ca7-be3a-de25c02df6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056c2ca-caa1-4e4f-90de-e375d65a325a",
   "metadata": {},
   "source": [
    "Now, set up an embedding model that will encode our content for the vector database. We will use FAISS to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705de6c7-0a0c-4e2c-a029-3ec151191e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm, trange # overcome notebook issue\n",
    "# sample documents\n",
    "documents = [\n",
    "    \"RAG combines retrieval-based methods with generation-based models for improved text generation.\",\n",
    "    \"It retrieves relevant information from a large corpus to enhance the generation process.\",\n",
    "    \"By using FAISS, we efficiently search over the document embeddings.\",\n",
    "    \"GPT models are commonly used for generating natural language responses.\",\n",
    "    \"Sentence-Transformers generate high-quality document embeddings.\"\n",
    "]\n",
    "\n",
    "# convert documents into embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c9df1e-2607-49b3-87a6-bcc03bc98ceb",
   "metadata": {},
   "source": [
    "Now we will add the documents to FAISS' index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d290dfbf-8ad2-41b3-b15e-988a0e9c5b66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "d = embeddings.shape[1]  # dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)  # L2 distance metric\n",
    "index.add(np.array(embeddings))  # add embeddings to the index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88351a6-b0ee-4325-b1c7-cccd75f4c11a",
   "metadata": {},
   "source": [
    "We are now ready to ask the LLM questions.\n",
    "This happens in the following steps:\n",
    "1. Ask our question (_'query'_)\n",
    "2. Create an embedding for our query. This will allow the vector database to search for similar documents.\n",
    "3. Run the search in the vector database.\n",
    "4. The search results will map to our document array, and will point at the documents the database found to be most relevant. \n",
    "5. We now know what documents to pass as context to the LLM.\n",
    "6. We combine our original query with the documents we identified to be relevant, and send them to the LLM.\n",
    "7. The LLM will read our documents (_'context'_) and compose a response to our query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf3fac7a-d5b3-45c9-a62a-da9fba26d8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of retrieved documents: [[1 0 3]]\n"
     ]
    }
   ],
   "source": [
    "# embed our query\n",
    "query = \"What is Retrieval-Augmented Generation?\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "# Run similarity search with FAISS\n",
    "k=3 # number of top results to retrieve\n",
    "distances, indices = index.search(np.array([query_embedding]), k)  \n",
    "\n",
    "# Print indices of the retrieved documents\n",
    "print(\"Indices of retrieved documents:\", indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67a68e-b46e-4d06-b812-5935d9d754f9",
   "metadata": {},
   "source": [
    "This result means that the second, first and fourth documents were most relevant for our query.\n",
    "\n",
    "We will now grab those documents from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd181db-85b8-4973-b787-43da3e72455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents: ['It retrieves relevant information from a large corpus to enhance the generation process.', 'RAG combines retrieval-based methods with generation-based models for improved text generation.', 'GPT models are commonly used for generating natural language responses.']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve actual document text based on FAISS indices\n",
    "retrieved_documents = [documents[i] for i in indices[0]]\n",
    "print(\"Retrieved documents:\", retrieved_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5ddc0-8886-4ea0-a2fc-1cb9a1b26872",
   "metadata": {},
   "source": [
    "We now need to combine these documents with our query so we send them as a block to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c87cf7-d1ed-4a64-bea2-c5ea4b3352f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input for GPT-2: What is Retrieval-Augmented Generation?\n",
      "It retrieves relevant information from a large corpus to enhance the generation process.\n",
      "RAG combines retrieval-based methods with generation-based models for improved text generation.\n",
      "GPT models are commonly used for generating natural language responses.\n"
     ]
    }
   ],
   "source": [
    "# Combine the query and retrieved documents into a single input for GPT-2\n",
    "input_text = query + \"\\n\" + \"\\n\".join(retrieved_documents)\n",
    "print(\"Input for GPT-2:\", input_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6508e7-f8ed-4d9f-8567-874cedc17c2e",
   "metadata": {},
   "source": [
    "With the input in hand, we need to tokenize it again for use by the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e3f460-52fd-418c-bba9-0123fa376ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: What is Retrieval-Augmented Generation?\n",
      "It retrieves relevant information from a large corpus to enhance the generation process.\n",
      "RAG combines retrieval-based methods with generation-based models for improved text generation.\n",
      "GPT models are commonly used for generating natural language responses. This provides a unique opportunity to use Retrieves, or Retrieve-Based Generative Learning, and to gain a better understanding of the underlying mechanisms behind the response.\n"
     ]
    }
   ],
   "source": [
    "# Set padding token to the eos_token_id\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "\n",
    "# Tokenize the input with padding\n",
    "inputs = gpt_tokenizer(input_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# This will provide both input_ids and attention_mask\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Generate response with the attention mask and padded input\n",
    "output = gpt_model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,  # Add the attention mask here\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    pad_token_id=gpt_tokenizer.eos_token_id  # Ensure pad token is set to eos_token\n",
    ")\n",
    "\n",
    "# Decode the generated output\n",
    "generated_response = gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Response:\", generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c183a7b-8549-4030-8092-e1e651572f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
