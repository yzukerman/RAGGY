{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c3eaf7-7aab-44a2-b713-e5b610e14913",
   "metadata": {},
   "source": [
    "# RAG with Weaviate and local embedding model\n",
    "\n",
    "## Overview\n",
    "In this chapter we will:\n",
    "1. Replace OpenAI's embedding model with a local one called ```nomic-embed-text```.\n",
    "2. Load the embedding into a new vector database (With the same structure).\n",
    "3. Query the database.\n",
    "4. Pass the result along with the query to the LLM\n",
    "We can then compare whether the results were any worse than those of the OpenAI embedding model.\n",
    "\n",
    "### A local embedding model\n",
    "As we saw in the previous chapter, OpenAI throttles our embedding and slows the process down. The rate appeared to be 5 embeddings per second. Not quick. In addition, OpenAI is also charging us for the pleasure. \n",
    "\n",
    "### Getting going\n",
    "[Ollama](https://ollama.com/) allows you to run LLMs locally. While I run on a 6 year-old Linux machine with an ancient AMD GPU, I am going to see if that's enough to host a small embedding model like Nomic.\n",
    "\n",
    "To run, download Ollama and [follow the instructions here](https://ollama.com/library/nomic-embed-text) on how to pull Nomic using Ollama's command line. \n",
    "\n",
    "### Create a new database\n",
    "We will start by creating a new collection/database in Weaviate. The database will have the same structure as we will still use Langchain's tools to read and split the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396dcdb4-6f1f-4857-9205-c1a004ce02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.config as wc\n",
    "import weaviate\n",
    "import os\n",
    "\n",
    "headers = {\n",
    "    \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "}  # Replace with your OpenAI API key\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"ADI_DOCS_TOO\",\n",
    "    properties=[\n",
    "        wc.Property(name=\"chunk_content\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"chunk_document_name\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"chunk_document_page\", data_type=wc.DataType.INT),\n",
    "    ],\n",
    "    # Define the vectorizer module\n",
    "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(),\n",
    "    # Define the generative module\n",
    "    generative_config=wc.Configure.Generative.openai()\n",
    ")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aae555-6eac-4374-82bf-1a5a15597569",
   "metadata": {},
   "source": [
    "## Load the chunks from the Pickle file\n",
    "To spare us the time and effort chunking our PDFs, let's load them from the file we saved in chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f050c41-964c-4b91-b439-6c1ef4ea0026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68448"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data from the pickle file\n",
    "with open(\"docs2/text_chunks.pkl\", \"rb\") as file:  # 'rb' means read in binary mode\n",
    "    chunks = pickle.load(file)\n",
    "\n",
    "# Now you can use the 'chunks' variable as needed\n",
    "len(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88427174-1724-4546-a81c-eef2fd5fc102",
   "metadata": {},
   "source": [
    "### Load the database with our local embedding model\n",
    "To try things out, let's start with adding a single chunk into Weaviate with Nomic embedding via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f5da540-86ed-4a8e-8c40-d47f8e2351df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01e14a17-6a7c-519a-8a0e-d40c1d8ac033\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "import ollama\n",
    "\n",
    "try:\n",
    "    # Connect to Weaviate\n",
    "    client = weaviate.connect_to_local()\n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "\n",
    "    chunk_obj = {\n",
    "                \"chunk_content\": chunks[0].page_content,\n",
    "                \"chunk_document_name\": chunks[0].metadata['source'],\n",
    "                \"chunk_document_page\": chunks[0].metadata['page'],\n",
    "            }\n",
    "    \n",
    "    # Create a UUID seed\n",
    "    cur_doc = chunks[0].metadata['source']\n",
    "    cur_page = chunks[0].metadata['page']\n",
    "\n",
    "    seed = cur_doc + \":\" + str(cur_page) + \":0\"\n",
    "\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", \n",
    "                                     prompt=chunks[0].page_content)\n",
    "\n",
    "    chunk_vector = response[\"embedding\"]\n",
    "    \n",
    "    uuid = adi_docs.data.insert(\n",
    "        properties = chunk_obj,\n",
    "        uuid= generate_uuid5(seed),\n",
    "        vector = chunk_vector\n",
    "    )\n",
    "\n",
    "    print(uuid)\n",
    "        \n",
    "        \n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426c632-197b-41fe-a1da-5b3fd5bf872a",
   "metadata": {},
   "source": [
    "That looks like it worked, but let's try to search for this. Since we brought our embedding, we need to embed our query ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12b072c-e10e-432f-bc67-54857ad096cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "W 5.0\n",
      "C/C++ Compiler and Library Manual\n",
      " for Blackfin® Processors\n",
      "Revision 5.4, January 2011\n",
      "Part Number\n",
      "82-000410-03\n",
      "Analog Devices, Inc.\n",
      "One T echnology Way\n",
      "Norwood, Mass. 02062-9106 01e14a17-6a7c-519a-8a0e-d40c1d8ac033\n",
      "Distance to query: 0.644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import weaviate.classes.query as wq\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "try:\n",
    "    # Connect to Weaviate\n",
    "    client = weaviate.connect_to_local()\n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "\n",
    "    # our query\n",
    "    query=\"EZ-Extender\"\n",
    "\n",
    "    # Get query embedding\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", \n",
    "                                     prompt=query)\n",
    "\n",
    "    query_vector = response[\"embedding\"]\n",
    "    \n",
    "    # Perform query\n",
    "    response = adi_docs.query.near_vector(\n",
    "        near_vector = query_vector, \n",
    "        limit=5, # maximum number of results\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    # Inspect the response\n",
    "    for o in response.objects:\n",
    "        print(\n",
    "            o.properties[\"chunk_content\"], o.uuid\n",
    "        )  # Print the title and release year (note the release date is a datetime object)\n",
    "        print(\n",
    "            f\"Distance to query: {o.metadata.distance:.3f}\\n\"\n",
    "        )  # Print the distance of the object from the query\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2938eb-ecde-4f00-b8fc-2e6d8cf198bb",
   "metadata": {},
   "source": [
    "#### Scaling that to the remaining chunks...\n",
    "Let's try a simplistic approach:\n",
    "1. Iterate over the chunk list\n",
    "2. Create a list of chunk objects (like we did above to hold the text, page, and source document)\n",
    "3. Create a list of corresponding embeddings with matching position IDs\n",
    "4. Create a list of UUIDs\n",
    "\n",
    "The goal will be to iterate over the two lists and batch insert the chunks into Weaviate. We will do that in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f396aa7-0ef3-4ae8-9d20-cf688fcee305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|███████████████████| 68447/68447 [11:05:36<00:00,  1.71chunk/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "chunk_count = len(chunks)\n",
    "chunk_obj_list = []\n",
    "chunk_embedding_list = []\n",
    "chunk_uuid_list = []\n",
    "\n",
    "# UUID generation\n",
    "# UUID seeds will be doc:page:chunk\n",
    "last_doc = \"\"\n",
    "last_page = \"\"\n",
    "page_chunk = 0\n",
    "\n",
    "# note that we are starting with chunk 1 \n",
    "# as we inserted chunk 0 manually above\n",
    "\n",
    "for i in tqdm(range(1, chunk_count), desc=\"Processing\", unit=\"chunk\"):\n",
    "\n",
    "    # Create a UUID seed\n",
    "    cur_doc = chunks[i].metadata['source']\n",
    "    cur_page = chunks[i].metadata['page']\n",
    "    cur_content = chunks[i].page_content\n",
    "    \n",
    "    chunk_obj = {\n",
    "                \"chunk_content\": cur_content,\n",
    "                \"chunk_document_name\": cur_doc,\n",
    "                \"chunk_document_page\": cur_page,\n",
    "            }\n",
    "\n",
    "    # Create a UUID seed\n",
    "    if last_doc != cur_doc:\n",
    "        last_doc = cur_doc\n",
    "\n",
    "    if last_page != cur_page:\n",
    "        last_page = cur_page\n",
    "        page_chunk = 0\n",
    "    else:\n",
    "        page_chunk += 1\n",
    "    \n",
    "    \n",
    "    seed = cur_doc + \":\" + str(cur_page) + \":\" + str(page_chunk)\n",
    "\n",
    "    # Generate embedding\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", \n",
    "                                     prompt=chunks[i].page_content)\n",
    "\n",
    "    chunk_vector = response[\"embedding\"]\n",
    "\n",
    "    # add to the lists!\n",
    "    chunk_obj_list.append(chunk_obj)\n",
    "    chunk_embedding_list.append(chunk_vector)\n",
    "    chunk_uuid_list.append(generate_uuid5(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f98e6-54ed-4b7e-a518-71f1f2ee47e7",
   "metadata": {},
   "source": [
    "As you can see, it does help to have a more powerful and modern machine with a GPU to run this process in less than 6 hours...\n",
    "For now, we will also save the vectors we produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22fe2428-ed84-4d00-9817-7e7715ff1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs2/text_vectors.pkl\", \"wb\") as file:  # 'wb' means write in binary mode\n",
    "    pickle.dump(chunk_embedding_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc466f-a628-45ba-b5c2-165e1ec1b879",
   "metadata": {},
   "source": [
    "### Load data into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8319055a-a646-4409-8be3-e7c2c90cb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████| 68447/68447 [00:27<00:00, 2533.47chunk/s]\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    # connect to database\n",
    "    client = weaviate.connect_to_local()\n",
    "           \n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "\n",
    "    list_pos = 0\n",
    "    \n",
    "    with adi_docs.batch.dynamic() as batch:\n",
    "           # Loop through the data\n",
    "        for chunk_obj in tqdm(chunk_obj_list, total=len(chunk_obj_list), desc=\"Processing\", unit=\"chunk\"):\n",
    "            cur_embedding = chunk_embedding_list[list_pos]\n",
    "            cur_uuid = chunk_uuid_list[list_pos]\n",
    "\n",
    "            # Add object to batch queue\n",
    "            batch.add_object(\n",
    "                properties=chunk_obj,\n",
    "                vector = cur_embedding,  \n",
    "                uuid=cur_uuid,              \n",
    "            )\n",
    "\n",
    "            list_pos += 1\n",
    "            \n",
    "      # Check for failed objects\n",
    "    if len(adi_docs.batch.failed_objects) > 0:\n",
    "        print(f\"Failed to import {len(adi_docs.batch.failed_objects)} objects\")\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e96ce-3f84-4af7-87d1-831c308b485c",
   "metadata": {},
   "source": [
    "Again, make sure we have all the chunks in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc0ccdf-fb72-4de2-8725-fdb3550b2545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68448\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    # connect to database\n",
    "    client = weaviate.connect_to_local()\n",
    "           \n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "    response = adi_docs.aggregate.over_all(total_count=True)\n",
    "    print(response.total_count)\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec4fbc4-06a6-438d-b144-747d9be998b2",
   "metadata": {},
   "source": [
    "### Perform RAG!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5deb4115-4f81-4065-ba39-234b831bc969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query: What is the most efficient method to manage memory and ensure optimal performance when using the ADSP-BF539’s Direct Memory Access (DMA) for continuous data transfers, and how can you avoid DMA aborts during high-priority tasks?\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter your query:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5798cb4-2650-446d-9179-179034d6311c",
   "metadata": {},
   "source": [
    "### Search the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7ea1f9-2003-44fc-82ba-ea67e2649c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZ-Extender. For more information on the jumper and connector settings \n",
      "required to power the extender, review “Power” on page 2-8  as well as \n",
      "“FPGA EZ-Extender Schematic” on page B-1 .\n",
      "Figure 1-1. FPGA EZ-Extender Setup d8a2b9f2-3438-51af-a39c-3236f1972f46\n",
      "Distance to query: 0.203\n",
      "\n",
      "EZ-Extender. For more information on the jumper and connector settings \n",
      "required to power the extender, review “Power” on page 2-8  as well as the \n",
      "schematics in Appendix C.\n",
      "Figure 1-1. FPGA EZ-Extender Setup a5b4354c-80a2-57da-9f70-e83e11f760f6\n",
      "Distance to query: 0.212\n",
      "\n",
      "a\n",
      "Probing EI3 Extender Board Manual \n",
      "an EZ-Extender® product \n",
      "Revision 1.0, August 2012\n",
      "Part Number  \n",
      "82-000243-01\n",
      "Analog Devices, Inc.\n",
      "One T echnology Way\n",
      "Norwood, Mass. 02062-9106 6ee8958d-ab05-547d-b9c7-b7940885cd76\n",
      "Distance to query: 0.229\n",
      "\n",
      "A-4 Blackfin EZ-Extender Manual 2ae69aff-b67f-5757-900c-b38d362cf868\n",
      "Distance to query: 0.235\n",
      "\n",
      "All of the power necessary to operate the extender is derived from the mat-\n",
      "ing EZ-KIT Lite/EZ-Board. Before using any of the interfaces, follow the \n",
      "setup procedure in “Bluetooth EZ-Extender Setup” on page 1-3 . e620f8be-77da-5b19-a11c-19e0d17513af\n",
      "Distance to query: 0.236\n",
      "\n",
      "EZ-Extender. For more information on the jumper and connector settings \n",
      "required to power the extender, review “Power” on page 2-8  as well as \n",
      "“FPGA EZ-Extender Schematic” on page B-1 .\n",
      "Figure 1-1. FPGA EZ-Extender Setup\n",
      "\n",
      "\n",
      "EZ-Extender. For more information on the jumper and connector settings \n",
      "required to power the extender, review “Power” on page 2-8  as well as the \n",
      "schematics in Appendix C.\n",
      "Figure 1-1. FPGA EZ-Extender Setup\n",
      "\n",
      "\n",
      "a\n",
      "Probing EI3 Extender Board Manual \n",
      "an EZ-Extender® product \n",
      "Revision 1.0, August 2012\n",
      "Part Number  \n",
      "82-000243-01\n",
      "Analog Devices, Inc.\n",
      "One T echnology Way\n",
      "Norwood, Mass. 02062-9106\n",
      "\n",
      "\n",
      "A-4 Blackfin EZ-Extender Manual\n",
      "\n",
      "\n",
      "All of the power necessary to operate the extender is derived from the mat-\n",
      "ing EZ-KIT Lite/EZ-Board. Before using any of the interfaces, follow the \n",
      "setup procedure in “Bluetooth EZ-Extender Setup” on page 1-3 .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import os\n",
    "import weaviate.classes.query as wq\n",
    "\n",
    "context = \"\"\n",
    "\n",
    "try:    \n",
    "    # connect to database\n",
    "    client = weaviate.connect_to_local()\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "\n",
    "    # create an embedding for the query\n",
    "\n",
    "    # Get query embedding\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", \n",
    "                                     prompt=query)\n",
    "\n",
    "    query_vector = response[\"embedding\"]\n",
    "    \n",
    "    # Perform query\n",
    "    response = adi_docs.query.near_vector(\n",
    "        near_vector = query_vector, \n",
    "        limit=5, # maximum number of results\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    # Inspect the response\n",
    "    for o in response.objects:\n",
    "        print(\n",
    "            o.properties[\"chunk_content\"], o.uuid\n",
    "        )  # Print the title and release year (note the release date is a datetime object)\n",
    "        print(\n",
    "            f\"Distance to query: {o.metadata.distance:.3f}\\n\"\n",
    "        )  # Print the distance of the object from the query\n",
    "\n",
    "    for o in response.objects:\n",
    "        print(o.properties['chunk_content'])\n",
    "        print(\"\\n\")\n",
    "        context = context + o.properties['chunk_content'] + '\\n---\\n'\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6991de6-740e-45ce-ace4-23de57b573b2",
   "metadata": {},
   "source": [
    "### Populate our prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78c2b8d-6caf-46d0-b533-94a54609045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = f\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "---\n",
    "Answer the question based on the above context: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aede78-bb08-44ed-8167-063c3cce2083",
   "metadata": {},
   "source": [
    "### Send query to OpenAI for generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8aae51b-1fa0-43f3-a2ea-346d6e8a7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: What product is being discussed in the given context?\n",
      "\n",
      "EZ-Extender\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "MODEL=\"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "completion = client.completions.create(\n",
    "  model = MODEL,\n",
    "    prompt = PROMPT_TEMPLATE,\n",
    "    max_tokens = 500,\n",
    "    temperature = 0.7,\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a31e99-0ee1-4287-b433-30ff51f28f4c",
   "metadata": {},
   "source": [
    "We can compare this answer to running it with ChatGPT4o, which answered:\n",
    "> To answer the question **\"What is the most efficient method to manage memory and ensure optimal performance when using the ADSP-BF539’s Direct Memory Access (DMA) for continuous data transfers, and how can you avoid DMA aborts during high-priority tasks?\"**, we can break it down based on key elements from the document.\n",
    "\n",
    "> ### 1. **Optimizing Memory Management for DMA Transfers:**\n",
    ">   - **Proper Allocation of Data Buffers:** A key technique is to **avoid memory conflicts** by carefully allocating data buffers within banks and pages of memory. The use of **autobuffering** or **descriptors** can reduce the need for unnecessary MMR (memory-mapped register) accesses, which can stall the DMA【156:0†source】.\n",
    ">   - **Minimize Direction Changes (Thrashing):** The processor's **traffic control features** help reduce efficiency losses caused by direction changes (i.e., switching from read to write or vice versa). Traffic control mechanisms, such as **DMACx_TC_PER** and **DMACx_TC_CNT** registers, can automatically optimize transfer direction and limit real-time processor intervention【156:0†source】【156:17†source】.\n",
    "\n",
    "> ### 2. **Ensuring Optimal Performance:**\n",
    ">   - **Channel Prioritization:** The ADSP-BF539 DMA channels are serviced based on priority, where lower-numbered channels get higher priority. Thus, peripherals requiring low-latency or high-throughput should be assigned to **lower-numbered DMA channels** via **DMAx_PERIPHERAL_MAP** registers【156:0†source】. This allows these peripherals to avoid being starved of DMA access by less important tasks.\n",
    ">   - **Round Robin Scheduling for MDMA:** When multiple memory DMA (MDMA) streams are enabled, the **MDMA_ROUND_ROBIN_PERIOD** can be configured to ensure that all streams share bandwidth fairly. This helps prevent starvation of lower-priority streams while maintaining high throughput【156:0†source】【156:4†source】.\n",
    "\n",
    ">### 3. **Avoiding DMA Aborts:**\n",
    ">  - **Error Handling:** The **DMA_ERROR interrupt** is triggered when certain conditions are detected, such as disallowed register writes, address alignment errors, or access to unpopulated memory spaces【156:2†source】【156:4†source】. To prevent these issues, ensure the **DMA_CONFIG register** is properly set and that memory addresses are aligned according to the specified word size. Additionally, **descriptors** should be configured correctly, especially when crossing memory boundaries【156:6†source】.\n",
    ">   - **MDMA Stream Configuration:** Ensure that **source and destination streams** are properly synchronized. Errors can occur if the configuration of word sizes or memory spaces between source and destination is incorrect【156:18†source】.\n",
    "\n",
    ">### 4. **Handling High-Priority Tasks:**\n",
    ">   - **Interrupt-based Synchronization:** It’s crucial to synchronize the completion of DMA tasks with software using interrupts rather than polling, which can lead to delays. Polling is not recommended due to pipeline and FIFO delays, which can result in synchronization issues between core and DMA operations【156:13†source】.\n",
    ">   - **Handling DMA Latency:** To avoid high-latency issues, careful arbitration must be managed between the **DMA controllers** and core processor when accessing internal memory. Ensure that **peripheral DMA** does not interfere with memory DMA, which can lead to **underflow** or **overflow** conditions【156:5†source】.\n",
    "\n",
    "> ### Conclusion:\n",
    ">Efficiently managing memory during DMA operations in the ADSP-BF539 requires careful **buffer allocation**, **traffic control**, and **prioritization** of high-priority channels. Additionally, preventing DMA aborts can be achieved through correct **configuration** of descriptors and **synchronization** using interrupts rather than polling.\n",
    "\n",
    ">By leveraging **round-robin scheduling**, **traffic control mechanisms**, and **interrupt-based synchronization**, you can maximize DMA performance while minimizing the risk of errors and aborts during high-priority tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868dcf40-1cfa-425e-8f4c-9c1f2cd46866",
   "metadata": {},
   "source": [
    "For giggles, let's try our prompt with a higher end model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f5e062-ce5a-4e0a-a5c1-50f705ea3afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The EZ-Extender is a product that can be powered by the mating EZ-KIT Lite/EZ-Board. For information on jumper and connector settings required to power the extender, you should review the sections titled “Power” on page 2-8, as well as the “FPGA EZ-Extender Schematic” on page B-1, and the schematics in Appendix C. Additionally, before using any interfaces, the setup procedure in “Bluetooth EZ-Extender Setup” on page 1-3 should be followed. The document references a \"Probing EI3 Extender Board Manual\" for the EZ-Extender with revision 1.0 from August 2012, part number 82-000243-01, by Analog Devices, Inc.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "MODEL=\"gpt-4o\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model = MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant with expertise in electrical engineering.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": PROMPT_TEMPLATE\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd8036-6ce1-4ff1-b0d7-a37697460f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
