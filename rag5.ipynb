{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c3eaf7-7aab-44a2-b713-e5b610e14913",
   "metadata": {},
   "source": [
    "# RAG with Weaviate and local embedding model\n",
    "\n",
    "## Overview\n",
    "In this chapter we will:\n",
    "1. Replace OpenAI's embedding model with a local one called ```nomic-embed-text```.\n",
    "2. Load the embedding into a new vector database (With the same structure).\n",
    "3. Query the database.\n",
    "4. Pass the result along with the query to the LLM\n",
    "We can then compare whether the results were any worse than those of the OpenAI embedding model.\n",
    "\n",
    "### A local embedding model\n",
    "As we saw in the previous chapter, OpenAI throttles our embedding and slows the process down. The rate appeared to be 5 embeddings per second. Not quick. In addition, OpenAI is also charging us for the pleasure. \n",
    "\n",
    "### Getting going\n",
    "[Ollama](https://ollama.com/) allows you to run LLMs locally. While I run on a 6 year-old Linux machine with an ancient AMD GPU, I am going to see if that's enough to host a small embedding model like Nomic.\n",
    "\n",
    "To run, download Ollama and [follow the instructions here](https://ollama.com/library/nomic-embed-text) on how to pull Nomic using Ollama's command line. \n",
    "\n",
    "### Create a new database\n",
    "We will start by creating a new collection/database in Weaviate. The database will have the same structure as we will still use Langchain's tools to read and split the PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396dcdb4-6f1f-4857-9205-c1a004ce02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.config as wc\n",
    "import weaviate\n",
    "import os\n",
    "\n",
    "headers = {\n",
    "    \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "}  # Replace with your OpenAI API key\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"ADI_DOCS_TOO\",\n",
    "    properties=[\n",
    "        wc.Property(name=\"chunk_content\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"chunk_document_name\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"chunk_document_page\", data_type=wc.DataType.INT),\n",
    "    ],\n",
    "    # Define the vectorizer module\n",
    "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(),\n",
    "    # Define the generative module\n",
    "    generative_config=wc.Configure.Generative.openai()\n",
    ")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e2479-4680-45ec-bfae-2a6256a46589",
   "metadata": {},
   "source": [
    "### Extract text from the PDFs\n",
    "Again, like in the previous chapter, we will repeat the text extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975c1810-7abc-4e00-af72-cf27dd410f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='aBlackfin速 A-V EZ-Extender速\n",
      "Manual\n",
      "Revision 2.1, July 2012\n",
      "Part Number\n",
      "82-000870-01\n",
      "Analog Devices, Inc.\n",
      "One T echnology Way\n",
      "Norwood, Mass. 02062-9106' metadata={'source': 'docs/AV_Blkf_EZ_extender_man_rev.2.1.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# load the documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(\"docs\")\n",
    "    return document_loader.load()\n",
    "\n",
    "# split documents to managable chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 800,\n",
    "        chunk_overlap = 80,\n",
    "        length_function = len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2678c091-1037-405a-8900-61e5b8931b85",
   "metadata": {},
   "source": [
    "Because this takes a while... let's save the chunks to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06053218-1f91-4fb6-922c-4afe22a77903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"docs/text_chunks.pkl\", \"wb\") as file:  # 'wb' means write in binary mode\n",
    "    pickle.dump(chunks, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a65e4d-c1f6-4e2a-88dc-e38c97d57f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 yuvalzukerman yuvalzukerman 45M Oct 21 19:56 docs/text_chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls -alh \"docs/text_chunks.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88427174-1724-4546-a81c-eef2fd5fc102",
   "metadata": {},
   "source": [
    "### Load the database with our local embedding model\n",
    "To try things out, let's start with adding a single chunk into Weaviate with Nomic embedding via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f5da540-86ed-4a8e-8c40-d47f8e2351df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3a0fe015-49b5-55aa-8d72-c1abbbb2b499\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import generate_uuid5\n",
    "import ollama\n",
    "\n",
    "try:\n",
    "    # Connect to Weaviate\n",
    "    client = weaviate.connect_to_local()\n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "\n",
    "    chunk_obj = {\n",
    "                \"chunk_content\": chunks[0].page_content,\n",
    "                \"chunk_document_name\": chunks[0].metadata['source'],\n",
    "                \"chunk_document_page\": chunks[0].metadata['page'],\n",
    "            }\n",
    "    \n",
    "    # Create a UUID seed\n",
    "    cur_doc = chunks[0].metadata['source']\n",
    "    cur_page = chunks[0].metadata['page']\n",
    "\n",
    "    seed = cur_doc + \":\" + str(cur_page) + \":0\"\n",
    "\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", \n",
    "                                     prompt=chunks[0].page_content)\n",
    "\n",
    "    chunk_vector = response[\"embedding\"]\n",
    "    \n",
    "    uuid = adi_docs.data.insert(\n",
    "        properties = chunk_obj,\n",
    "        uuid= generate_uuid5(seed),\n",
    "        vector = chunk_vector\n",
    "    )\n",
    "\n",
    "    print(uuid)\n",
    "        \n",
    "        \n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426c632-197b-41fe-a1da-5b3fd5bf872a",
   "metadata": {},
   "source": [
    "That looks like it worked, but let's try to search for this. Since we brought our embedding, we need to embed our query ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e12b072c-e10e-432f-bc67-54857ad096cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aBlackfin速 A-V EZ-Extender速\n",
      "Manual\n",
      "Revision 2.1, July 2012\n",
      "Part Number\n",
      "82-000870-01\n",
      "Analog Devices, Inc.\n",
      "One T echnology Way\n",
      "Norwood, Mass. 02062-9106 3a0fe015-49b5-55aa-8d72-c1abbbb2b499\n",
      "Distance to query: 0.285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import weaviate.classes.query as wq\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "try:\n",
    "    # Connect to Weaviate\n",
    "    client = weaviate.connect_to_local()\n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS_TOO\")\n",
    "\n",
    "    # our query\n",
    "    query=\"EZ-Extender\"\n",
    "\n",
    "    # Get query embedding\n",
    "    response = ollama.embeddings(model=\"nomic-embed-text\", \n",
    "                                     prompt=query)\n",
    "\n",
    "    query_vector = response[\"embedding\"]\n",
    "    \n",
    "    # Perform query\n",
    "    response = adi_docs.query.near_vector(\n",
    "        near_vector = query_vector, \n",
    "        limit=5, # maximum number of results\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    # Inspect the response\n",
    "    for o in response.objects:\n",
    "        print(\n",
    "            o.properties[\"chunk_content\"], o.uuid\n",
    "        )  # Print the title and release year (note the release date is a datetime object)\n",
    "        print(\n",
    "            f\"Distance to query: {o.metadata.distance:.3f}\\n\"\n",
    "        )  # Print the distance of the object from the query\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2938eb-ecde-4f00-b8fc-2e6d8cf198bb",
   "metadata": {},
   "source": [
    "#### Scaling that to the remaining chunks...\n",
    "Let's try a simplistic approach:\n",
    "1. Iterate over the chunk list\n",
    "2. Create a list of chunk objects (like we did above to hold the text, page, and source document)\n",
    "3. Create a list of corresponding embeddings with matching position IDs\n",
    "\n",
    "The goal will be to iterate over the two lists and batch insert the chunks into Weaviate. We will do that in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f396aa7-0ef3-4ae8-9d20-cf688fcee305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
