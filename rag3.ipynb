{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23485bf3-abcc-4b30-8158-76f38990130f",
   "metadata": {},
   "source": [
    "## Use PDF documents with RAG\n",
    "This tutorial downloads a set of PDF files from the Analog Devices website. You can use your own later. We will use these documents as our RAG knowledge base, with the goal of asking the LLM questions about information they contain.\n",
    "\n",
    "In this tutorial:\n",
    "1. Download the documents\n",
    "2. We will extract text from the documents and break it apart into chunks\n",
    "3. Vectorize the text chunks so the vector database can search them efficiently (semantic search)\n",
    "4. Load the vectors and text chunks into the database.\n",
    "\n",
    "### Document download\n",
    "We will download documents from [this page](https://www.analog.com/en/lp/001/blackfin-manuals.html) using Pandas' awesome capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6c38f9f-8a7a-4f39-a431-8fddeb079e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Downloading 'https://www.analog.com/en/lp/001/blackfin-manuals.html' ...\n",
      "Saving 'blackfin-manuals.html'\n",
      "HTTP response 200  [https://www.analog.com/en/lp/001/blackfin-manuals.html]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████| 91/91 [00:12<00:00,  7.40files/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "url = \"https://www.analog.com/en/lp/001/blackfin-manuals.html\"\n",
    "directory = \"docs2\"\n",
    "\n",
    "try:\n",
    "    # download the file\n",
    "    subprocess.run([\"wget\", url])\n",
    "    parsed_url = urlparse(url)\n",
    "    file_name = os.path.basename(parsed_url.path)\n",
    "    # open the file\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "    \n",
    "    # parse the webpage\n",
    "    soup = BeautifulSoup(file_content, \"html.parser\")\n",
    "    \n",
    "    # find pdf links\n",
    "    pdf_links = []\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "        if href.endswith(\".pdf\"):\n",
    "            pdf_links.append(href)\n",
    "\n",
    "    # make directory to store the pdfs in\n",
    "    os.makedirs(directory, exist_ok = True)\n",
    "    \n",
    "    # download pdfs\n",
    "    for cur_pdf in tqdm(pdf_links, total=len(pdf_links), desc=\"Downloading\", unit=\"files\"):\n",
    "        response = requests.get(cur_pdf)\n",
    "        # get file name\n",
    "        parsed_url = urlparse(cur_pdf)\n",
    "        file_name = target_directory + \"/\" + os.path.basename(parsed_url.path)     \n",
    "        with open(file_name, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except Exception as err:\n",
    "    print(f\"An error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aec92f-8821-4cd5-83c1-71c93164bbb1",
   "metadata": {},
   "source": [
    "### Break documents into text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afdcfc84-e876-43a3-be1b-e6f2a6dbc555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='ADSP-BF7xx Blackfin+ Processor\n",
      "Programming Reference\n",
      "Revision 1.0, October 2016\n",
      "Part Number\n",
      "82-100123-01\n",
      "Analog Devices, Inc.\n",
      "Three T echnology Way\n",
      "Norwood MA , 02062' metadata={'source': 'docs2/ADSP-BF70x_Blackfin_Programming_Reference.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# load the documents\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(\"docs2\")\n",
    "    return document_loader.load()\n",
    "\n",
    "# split documents to managable chunks\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 800,\n",
    "        chunk_overlap = 80,\n",
    "        length_function = len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "documents = load_documents()\n",
    "chunks = split_documents(documents)\n",
    "print(chunks[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e718fef9-28da-4e92-8634-40f7721cadae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='(W32)\n",
      "Input data operand format is signed fractional with no extension bits in the accumulators at 32 bits. Left-shift\n",
      "correction of the product is performed, as required. This option is used for legacy GSM speech vocoder algo-\n",
      "rithms written for 32-bit accumulators. For this option only, this special case applies:\n",
      "0x8000 x 0x8000 = 0x7FFF\n",
      "(M)\n",
      "Operation uses mixed-multiply mode. Valid only for MAC1 versions of the instruction. Multiplies a signed\n",
      "fractional operand by an unsigned fractional operand with no left-shift correction, where the first operand is\n",
      "signed and the second is unsigned. MAC0 performs an unmixed multiplication of signed fractional operands\n",
      "unless another format as specified (i.e., MAC0 executes the specified signed/signed or unsigned/unsigned mul-' metadata={'source': 'docs2/ADSP-BF70x_Blackfin_Programming_Reference.pdf', 'page': 78}\n"
     ]
    }
   ],
   "source": [
    "print (chunks[330])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a26ccba-e8e8-4ff3-bb2b-d40f8ee4d03e",
   "metadata": {},
   "source": [
    "Let's check how many chunks were generated from our documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f96db366-22eb-4fd6-baef-80a0fef304db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68386"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6d8e3a2-f5fc-4d5e-abdc-3879e49fe6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(W32)\n",
      "Input data operand format is signed fractional with no extension bits in the accumulators at 32 bits. Left-shift\n",
      "correction of the product is performed, as required. This option is used for legacy GSM speech vocoder algo-\n",
      "rithms written for 32-bit accumulators. For this option only, this special case applies:\n",
      "0x8000 x 0x8000 = 0x7FFF\n",
      "(M)\n",
      "Operation uses mixed-multiply mode. Valid only for MAC1 versions of the instruction. Multiplies a signed\n",
      "fractional operand by an unsigned fractional operand with no left-shift correction, where the first operand is\n",
      "signed and the second is unsigned. MAC0 performs an unmixed multiplication of signed fractional operands\n",
      "unless another format as specified (i.e., MAC0 executes the specified signed/signed or unsigned/unsigned mul-\n",
      "docs2/ADSP-BF70x_Blackfin_Programming_Reference.pdf\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "print(chunks[330].page_content)\n",
    "print(chunks[330].metadata['source'])\n",
    "print(chunks[330].metadata['page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78338f95-b54b-405e-85bd-895f7d387d70",
   "metadata": {},
   "source": [
    "Let's save the chunks in case we need them again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80e8de91-fc93-4600-9c11-b771840ab478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"docs2/text_chunks.pkl\", \"wb\") as file:  # 'wb' means write in binary mode\n",
    "    pickle.dump(chunks, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c89720-5a03-45e2-9710-d418e28ef143",
   "metadata": {},
   "source": [
    "### Embedding the chunks\n",
    "Now we need to embed the chunks in the database.\n",
    "\n",
    "As you can see, each chunk consists of [Langchain Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document). \n",
    "Each Document has two elements:\n",
    "* ```page_content``` - the text of the chunk\n",
    "* ```metadata``` - a dictionary object, which in our case, contains the source document path and the page in which the chunk appeared.\n",
    "\n",
    "We will need to store this information in our vector database, Weaviate. Therefore, we will need to start with a very simple schema.\n",
    "* chunk_content\n",
    "* chunk_document_name\n",
    "* chunk_document_page\n",
    "\n",
    "Weaviate will handle the embedding for us using the model we specified (OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b742078e-929d-410b-b831-f7f52b3069ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_94b0657936662af5b43d0957fcf3ed0f error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 996519, Requested 88128. Please try again in 5.078s. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 62 seconds.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:228: DeprecationWarning: Dep020: The `all_responses` attribute in the `BatchResults` object is deprecated and will be removed by Q4 2024. Please instead use the `errors` and `uuids` attributes.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_9d96a6adcf4097be5ccf643861af8890 error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 935287, Requested 96374. Please try again in 1.899s. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 62 seconds.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_f4567adc27645c66e13dc22118281964 error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 933669, Requested 112444. Please try again in 2.766s. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 62 seconds.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_7e244466438d86151d59e63e70a8bc2a error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 935729, Requested 114488. Please try again in 3.013s. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 62 seconds.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_2b96cbd0b42629d02351a7ba3656bdfb error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 891547, Requested 114488. Please try again in 362ms. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 124 seconds.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_14b11f451429e9a5b58ecb2d6bc0d339 error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 963259, Requested 105583. Please try again in 4.13s. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 62 seconds.\n",
      "  warnings.warn(\n",
      "/home/yuvalzukerman/.local/lib/python3.12/site-packages/weaviate/warnings.py:312: UserWarning: Bat005: Rate limit reached with error WeaviateInsertManyAllFailedError('Every object failed during insertion. Here is the set of all errors: connection to: OpenAI API failed with status: 429 request-id: req_7e289b902e0e044b3896dbb4a4732c65 error: Rate limit reached for text-embedding-ada-002 in organization org-TpPP0kLjXaLunR4pFEPauR5c on tokens per min (TPM): Limit 1000000, Used 949795, Requested 103547. Please try again in 3.2s. Visit https://platform.openai.com/account/rate-limits to learn more.').\n",
      "            Sleeping for 62 seconds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate.classes.config as wc\n",
    "import weaviate\n",
    "import os\n",
    "\n",
    "headers = {\n",
    "    \"X-OpenAI-Api-Key\": os.getenv(\"OPENAI_API_KEY\")\n",
    "}  # Replace with your OpenAI API key\n",
    "\n",
    "client = weaviate.connect_to_local(headers=headers)\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"ADI_DOCS\",\n",
    "    properties=[\n",
    "        wc.Property(name=\"chunk_content\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"chunk_document_name\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"chunk_document_page\", data_type=wc.DataType.INT),\n",
    "    ],\n",
    "    # Define the vectorizer module\n",
    "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(),\n",
    "    # Define the generative module\n",
    "    generative_config=wc.Configure.Generative.openai()\n",
    ")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c081c8-2ecb-4170-9f29-9e1533f510aa",
   "metadata": {},
   "source": [
    "#### Load Weaviate with the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73a1fe2b-e31c-41b5-bfb7-6f59ebfcd81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 68090/68090 [10:48<00:00, 104.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "try:    \n",
    "    # connect to database\n",
    "    client = weaviate.connect_to_local(headers=headers)\n",
    "           \n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS\")\n",
    "    \n",
    "    cur_page = 0\n",
    "    cur_doc = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    # Enter context manager\n",
    "    with adi_docs.batch.dynamic() as batch:\n",
    "        # Loop through the data\n",
    "        for chunk in tqdm(chunks, total=len(chunks)):\n",
    "            i +=1\n",
    "            # Convert data types\n",
    "\n",
    "            chunk_obj = {\n",
    "                \"chunk_content\": chunk.page_content,\n",
    "                \"chunk_document_name\": chunk.metadata['source'],\n",
    "                \"chunk_document_page\": chunk.metadata['page'],\n",
    "            }\n",
    "\n",
    "            if cur_doc != chunk.metadata['source']:\n",
    "                cur_doc = chunk.metadata['source']\n",
    "\n",
    "            if cur_page != chunk.metadata['page']:\n",
    "                cur_page = chunk.metadata['page']\n",
    "                \n",
    "            seed = cur_doc + \":\" + str(cur_page) + \":\" + str(i)\n",
    "    \n",
    "            # Add object to batch queue\n",
    "            batch.add_object(\n",
    "                properties=chunk_obj,\n",
    "                uuid=generate_uuid5(seed)\n",
    "                # references=reference_obj  # You can add references here\n",
    "            )\n",
    "            # Batcher automatically sends batches\n",
    "    \n",
    "    # Check for failed objects\n",
    "    if len(adi_docs.batch.failed_objects) > 0:\n",
    "        print(f\"Failed to import {len(adi_docs.batch.failed_objects)} objects\")\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b12dfb-29e8-4dd4-a7e6-b2e636ef3bf0",
   "metadata": {},
   "source": [
    "Let's verify that the records are in the vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1737e527-478b-4e0d-998e-2272f42c34f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68090\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    # connect to database\n",
    "    client = weaviate.connect_to_local(headers=headers)\n",
    "           \n",
    "    # Get the collection\n",
    "    adi_docs = client.collections.get(\"ADI_DOCS\")\n",
    "    response = adi_docs.aggregate.over_all(total_count=True)\n",
    "    print(response.total_count)\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b847bf4-aa4b-4593-aaca-35b9abe2c4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
